+++
date = "2017-09-12"
title = "HW4"
abstract = "Please try to complete before class on Thursday in case there are questions or clarifications needed."
abstract_short = "Please try to complete before class on Thursday in case there are questions or clarifications needed."

draft=false
selected = true
math = true

url_pdf = "http://getitatduke.library.duke.edu/?sid=sersol&SS_jc=TC0000508493&title=Plane%20Answers%20to%20Complex%20Questions%3A%20The%20Theory%20of%20Linear%20Models"
url_slides = ""
url_video = ""

# Optional featured image (relative to `static/img/` folder).
[header]
image = "headers/Bayes_Theorem.jpg"
caption = "https://commons.wikimedia.org/wiki/File:Bayes%27_Theorem_MMB_01.jpg"

+++
**Due Friday 09/15/2017 11:55:00 PM**

Please try to complete before class on Thursday in case there are questions or clarifications needed (or post on Piazza). Use LaTeX or write by hand (must be legible) and scan to submit via Sakai.  

 Consider the linear model $Y = X\beta + \epsilon$ with $E[\epsilon] = 0_n$ and Cov$(\epsilon) = \sigma^2 I_n$ and with $X$ of full
column rank $p$.  Answer the following questions.  _Hint:  rewrite each as a quadratic form_

1. Consider estimation of $\beta$ using quadratic loss $(\beta-
  a)^T(\beta- a)$ for some estimator $a$.  Find the expected quadratic
  loss if we use the MLE $\hat{\beta}$ for $a$ conditional on $X$. Simplify the expression
  as a function of the eigenvalues of $X^TX$.   What happens as the
  smallest eigenvalue of $X^TX$ goes to $0$?

2. Consider estimation of $\mu$ at the observed data points
  $X$.  Find the expected  quadratic loss  
  $E[(\mu - X\hat{\beta})^T(\mu - X\hat{\beta})]$ conditional on $X$.  What happens as the  smallest eigenvalue of $X^TX$ goes to 0?
  
3. Consider predicting a new $Y\_{\*}$ at the observed data points $X$
  where $Y\_{\*}$ is independent of $Y$.  Find the expected quadratic
  loss for $E[(Y\_{\*} - X\hat{\beta})^T(Y\_{\*} - X\hat{\beta})]$.  What happens as the
  smallest eigenvalue of $X^TX$ goes to 0? 

4. Consider predicting $Y\_{\*}$'s at new points $X\_{\*}$ with
  $E[X\_{\*}^TX\_{\*}] = I\_p$.  Find the expected quadratic loss
  $E[(Y\_{\*} - X\_{\*}\hat{\beta})^T(Y\_{\*} - X\_{\*}\hat{\beta})]$ conditional on $X$ and $X\_{\*}$ and then unconditional on $X\_{\*}$ (but still conditional on $X$.  What
  happens as the smallest eigenvalue of $X^TX$ goes to 0?  (If
  $E[X\_{\*}^TX\_{\*}] = \Sigma > 0$ does that change the result)

5. Comment on the difference in estimation and prediction at observed
  data versus new data as $X$ becomes non-full rank.
  Which is the most stable?  Which is the least?



Review Chapter 2 and Appendix C in [Plane Answers to Complex Questions](http://getitatduke.library.duke.edu/?sid=sersol&SS_jc=TC0000508493&title=Plane%20Answers%20to%20Complex%20Questions%3A%20The%20Theory%20of%20Linear%20Models)

